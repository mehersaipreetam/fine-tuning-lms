# Fine-Tuning Language Models

This project demonstrates the fine-tuning of large language models using **LoRA (Low-Rank Adaptation)** with the **Hugging Face Transformers** library. LoRA is a parameter-efficient fine-tuning technique that adapts large models by introducing low-rank updates to the model weights, significantly reducing the number of trainable parameters while maintaining high performance.

## Notebooks

1. **Fine-Tuning LMs.ipynb**: This notebook walks through the process of fine-tuning a large language model using LoRA, explaining the steps to apply LoRA, load data, and train the model efficiently.
   
2. **Understanding Hugging Face.ipynb**: This notebook provides an introduction to using the Hugging Face `transformers` library.
